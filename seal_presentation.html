<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>SEAL: Self-Adapting Language Models</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            margin: 0;
            padding: 0;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: #333;
        }
        
        .slide-container {
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
        }
        
        .slide {
            background: white;
            border-radius: 15px;
            padding: 40px;
            margin: 30px 0;
            box-shadow: 0 10px 30px rgba(0,0,0,0.2);
            min-height: 500px;
            display: none;
        }
        
        .slide.active {
            display: block;
        }
        
        .slide h1 {
            color: #2c3e50;
            border-bottom: 3px solid #3498db;
            padding-bottom: 10px;
            margin-bottom: 30px;
            font-size: 2.5em;
        }
        
        .slide h2 {
            color: #34495e;
            margin-top: 30px;
            margin-bottom: 20px;
            font-size: 1.8em;
        }
        
        .slide h3 {
            color: #7f8c8d;
            margin-bottom: 15px;
            font-size: 1.3em;
        }
        
        .challenge-box, .method-box, .result-box {
            background: #f8f9fa;
            border-left: 5px solid #3498db;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }
        
        .method-box {
            border-left-color: #e74c3c;
        }
        
        .result-box {
            border-left-color: #27ae60;
        }
        
        .two-column {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 20px 0;
        }
        
        .three-column {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 20px;
            margin: 20px 0;
        }
        
        .navigation {
            text-align: center;
            margin: 20px 0;
        }
        
        .nav-btn {
            background: #3498db;
            color: white;
            border: none;
            padding: 12px 24px;
            margin: 0 10px;
            border-radius: 8px;
            cursor: pointer;
            font-size: 16px;
            transition: background 0.3s;
        }
        
        .nav-btn:hover {
            background: #2980b9;
        }
        
        .nav-btn:disabled {
            background: #bdc3c7;
            cursor: not-allowed;
        }
        
        .slide-counter {
            text-align: center;
            color: white;
            font-size: 18px;
            margin: 20px 0;
        }
        
        ul {
            line-height: 1.8;
        }
        
        li {
            margin: 10px 0;
        }
        
        .highlight {
            background: #fff3cd;
            padding: 15px;
            border-radius: 8px;
            border-left: 4px solid #ffc107;
            margin: 20px 0;
        }
        
        .algorithm-box {
            background: #e8f5e8;
            border: 2px solid #4caf50;
            padding: 20px;
            border-radius: 10px;
            margin: 20px 0;
            font-family: 'Courier New', monospace;
        }
        
        .performance-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        
        .performance-table th, .performance-table td {
            border: 1px solid #ddd;
            padding: 12px;
            text-align: center;
        }
        
        .performance-table th {
            background: #3498db;
            color: white;
        }
        
        .performance-table tr:nth-child(even) {
            background: #f2f2f2;
        }
        
        .best-result {
            background: #d4edda !important;
            font-weight: bold;
        }
    </style>
</head>
<body>
    <div class="slide-container">
        <div class="slide-counter">
            <span id="current-slide">1</span> / <span id="total-slides">12</span>
        </div>
        
        <!-- Slide 1: Title -->
        <div class="slide active">
            <h1>SEAL: Self-Adapting Language Models</h1>
            <div style="text-align: center; margin: 40px 0;">
                <h2>A Framework for LLMs to Generate Their Own Training Data</h2>
                <p style="font-size: 1.2em; color: #7f8c8d; margin: 30px 0;">
                    Adam Zweiger, Jyothish Pari, Han Guo, Ekin Aky√ºrek, Yoon Kim, Pulkit Agrawal<br>
                    Massachusetts Institute of Technology
                </p>
                <div class="highlight">
                    <strong>Key Innovation:</strong> LLMs that can self-adapt by generating their own finetuning data and update directives through reinforcement learning
                </div>
            </div>
        </div>

        <!-- Slide 2: Motivation & Challenges -->
        <div class="slide">
            <h1>Motivation & Challenges</h1>
            
            <h2>üéØ The Problem</h2>
            <div class="challenge-box">
                <h3>Current LLMs are Static</h3>
                <ul>
                    <li>Powerful but lack mechanisms to adapt weights in response to new tasks/knowledge</li>
                    <li>Consume and learn from task data "as-is" via finetuning or in-context learning</li>
                    <li>Cannot develop bespoke strategies for transforming and learning from data</li>
                </ul>
            </div>

            <h2>üß† Human Learning Analogy</h2>
            <div class="two-column">
                <div>
                    <h3>Students preparing for exams:</h3>
                    <ul>
                        <li>Don't just memorize raw content</li>
                        <li>Rewrite information as notes</li>
                        <li>Create diagrams, summaries</li>
                        <li>Transform data for better understanding</li>
                    </ul>
                </div>
                <div>
                    <h3>Current LLMs:</h3>
                    <ul>
                        <li>Learn from data "as-is"</li>
                        <li>No self-transformation</li>
                        <li>No personalized learning strategies</li>
                        <li>Static adaptation approaches</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Slide 3: Goal & Vision -->
        <div class="slide">
            <h1>Research Goal & Vision</h1>
            
            <h2>üéØ Primary Goal</h2>
            <div class="highlight">
                <strong>Enable LLMs to self-adapt by generating their own training data and finetuning directives</strong>
            </div>

            <h2>üîÆ Vision: Self-Adapting LLMs (SEAL)</h2>
            <div class="method-box">
                <h3>Key Capabilities:</h3>
                <ul>
                    <li><strong>Self-Edit Generation:</strong> Create synthetic training data from new information</li>
                    <li><strong>Optimization Control:</strong> Specify their own hyperparameters and training procedures</li>
                    <li><strong>Persistent Learning:</strong> Achieve lasting weight updates through supervised finetuning</li>
                    <li><strong>Reward-Driven Improvement:</strong> Learn better self-editing through RL</li>
                </ul>
            </div>

            <h2>üöÄ Applications</h2>
            <div class="two-column">
                <div>
                    <h3>Knowledge Incorporation</h3>
                    <p>Integrate new factual information into model weights without relying on context</p>
                </div>
                <div>
                    <h3>Few-Shot Learning</h3>
                    <p>Automatically configure adaptation pipeline for novel tasks with minimal examples</p>
                </div>
            </div>
        </div>

        <!-- Slide 4: Method Overview -->
        <div class="slide">
            <h1>SEAL Method Overview</h1>
            
            <h2>üîÑ Two-Loop Architecture</h2>
            <div class="two-column">
                <div class="method-box">
                    <h3>Outer Loop: Reinforcement Learning</h3>
                    <ul>
                        <li>Optimizes self-edit generation policy</li>
                        <li>Uses downstream performance as reward</li>
                        <li>Trains model to produce better self-edits</li>
                    </ul>
                </div>
                <div class="method-box">
                    <h3>Inner Loop: Weight Updates</h3>
                    <ul>
                        <li>Applies generated self-edits</li>
                        <li>Updates model via gradient descent</li>
                        <li>Uses LoRA for efficient adaptation</li>
                    </ul>
                </div>
            </div>

            <div class="algorithm-box">
                <h3>SEAL Algorithm:</h3>
                <pre>1. Sample context (C, œÑ) from dataset
2. Generate self-edit SE ~ LM(¬∑|C)
3. Inner Loop: Œ∏' ‚Üê SFT(Œ∏, SE)
4. Evaluate: performance on œÑ
5. Compute reward r based on improvement
6. Update policy: Œ∏ ‚Üê RL_Update(Œ∏, r, SE)</pre>
            </div>

            <div class="highlight">
                <strong>Key Innovation:</strong> The model directly uses its own generation to parameterize and control its adaptation process
            </div>
        </div>

        <!-- Slide 5: Method - Self-Edit Generation -->
        <div class="slide">
            <h1>Method: Self-Edit Generation</h1>
            
            <h2>üìù Definition of Self-Edits</h2>
            <div class="method-box">
                <p><strong>From the paper:</strong></p>
                <blockquote style="font-style: italic; background: #f0f8ff; padding: 15px; border-left: 4px solid #3498db; margin: 15px 0;">
                "Self-edits are natural-language instructions that specify the data and, optionally, the optimization hyperparameters for updating the model's weights"
                </blockquote>
                <p><strong>Key characteristics:</strong></p>
                <ul>
                    <li>Generated directly through token generation with data in context</li>
                    <li>Can restructure information in different ways</li>
                    <li>May specify optimization hyperparameters</li>
                    <li>Can invoke tools for data augmentation and gradient-based updates</li>
                </ul>
            </div>

            <h2>üîß Domain-Specific Examples (From Paper Figures)</h2>
            <div class="two-column">
                <div>
                    <h3>Knowledge Incorporation (Figure 2)</h3>
                    <div style="border: 2px solid #3498db; padding: 15px; background: #f8f9fa; border-radius: 8px;">
                        <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='500' height='200' viewBox='0 0 500 200'%3E%3Crect width='500' height='200' fill='%23f8f9fa'/%3E%3Ctext x='250' y='30' text-anchor='middle' font-size='16' font-weight='bold'%3EFigure 2: Knowledge Incorporation Setup%3C/text%3E%3Ctext x='20' y='60' font-size='12' font-weight='bold'%3EPassage:%3C/text%3E%3Ctext x='20' y='80' font-size='10'%3Etitle: Apollo program%3C/text%3E%3Ctext x='20' y='95' font-size='10'%3Econtext: But even after NASA reached%3C/text%3E%3Ctext x='20' y='110' font-size='10'%3Einternal agreement, it was far from%3C/text%3E%3Ctext x='20' y='125' font-size='10'%3Esmooth sailing...%3C/text%3E%3Ctext x='200' y='60' font-size='12' font-weight='bold'%3ESelf-Edit:%3C/text%3E%3Ctext x='200' y='80' font-size='10'%3E1. The Apollo program faced%3C/text%3E%3Ctext x='200' y='95' font-size='10'%3Eopposition from Kennedy's science%3C/text%3E%3Ctext x='200' y='110' font-size='10'%3Eadvisor, Jerome Wiesner, who had...%3C/text%3E%3Ctext x='350' y='60' font-size='12' font-weight='bold'%3EEvaluation:%3C/text%3E%3Ctext x='350' y='80' font-size='10'%3EWho was Kennedy's science%3C/text%3E%3Ctext x='350' y='95' font-size='10'%3Eadviser that opposed manned%3C/text%3E%3Ctext x='350' y='110' font-size='10'%3Espacecraft flights?%3C/text%3E%3Ctext x='350' y='130' font-size='10' font-weight='bold'%3EJerome Wiesner%3C/text%3E%3C/svg%3E" alt="Knowledge Incorporation Example" style="width: 100%; max-width: 480px;">
                        <p style="margin-top: 10px; font-size: 0.9em;"><strong>Process:</strong> Passage ‚Üí Implications ‚Üí SFT ‚Üí Evaluation without passage</p>
                    </div>
                </div>
                <div>
                    <h3>Few-Shot Learning (Figure 3)</h3>
                    <div style="border: 2px solid #e74c3c; padding: 15px; background: #f8f9fa; border-radius: 8px;">
                        <img src="data:image/svg+xml,%3Csvg xmlns='http://www.w3.org/2000/svg' width='500' height='200' viewBox='0 0 500 200'%3E%3Crect width='500' height='200' fill='%23f8f9fa'/%3E%3Ctext x='250' y='20' text-anchor='middle' font-size='16' font-weight='bold'%3EFigure 3: Few-Shot Learning with SEAL%3C/text%3E%3Ctext x='20' y='45' font-size='12' font-weight='bold'%3EFew-Shot Examples:%3C/text%3E%3Crect x='20' y='50' width='15' height='15' fill='%234a90e2'/%3E%3Crect x='40' y='50' width='15' height='15' fill='%234a90e2'/%3E%3Crect x='20' y='70' width='15' height='15' fill='%23f39c12'/%3E%3Crect x='40' y='70' width='15' height='15' fill='%23f39c12'/%3E%3Ctext x='20' y='100' font-size='10'%3E(Input-Output Grid Examples)%3C/text%3E%3Ctext x='150' y='45' font-size='12' font-weight='bold'%3ESelf-Edit (SE):%3C/text%3E%3Ctext x='150' y='65' font-size='10'%3Ebasic_augmentations: true%3C/text%3E%3Ctext x='150' y='80' font-size='10'%3Esize_augmentations: false%3C/text%3E%3Ctext x='150' y='95' font-size='10'%3Echain_augmentations: false%3C/text%3E%3Ctext x='150' y='110' font-size='10'%3Erepeat_augmentations: false%3C/text%3E%3Ctext x='150' y='130' font-size='10'%3Estrategy: loss on all tokens%3C/text%3E%3Ctext x='150' y='145' font-size='10'%3Elearning rate: 1e-05%3C/text%3E%3Ctext x='150' y='160' font-size='10'%3Eepochs: 3%3C/text%3E%3Ctext x='350' y='45' font-size='12' font-weight='bold'%3EEvaluation:%3C/text%3E%3Crect x='350' y='50' width='15' height='15' fill='%2327ae60'/%3E%3Crect x='370' y='50' width='15' height='15' fill='%2327ae60'/%3E%3Ctext x='350' y='90' font-size='10'%3E(Test Input ‚Üí Correct Output)%3C/text%3E%3C/svg%3E" alt="Few-Shot Learning Example" style="width: 100%; max-width: 480px;">
                        <p style="margin-top: 10px; font-size: 0.9em;"><strong>Process:</strong> Examples ‚Üí Tool Configuration ‚Üí SFT ‚Üí Test Evaluation</p>
                    </div>
                </div>
            </div>

            <div class="highlight">
                <strong>Key Insight:</strong> Self-edits enable the model to control both what data to learn from and how to learn from it
            </div>
        </div>

        <!-- Slide 6: Method - Reinforcement Learning -->
        <div class="slide">
            <h1>Method: Reinforcement Learning Training</h1>
            
            <h2>üéØ RL Objective</h2>
            <div class="algorithm-box">
                <strong>Maximize Expected Reward:</strong><br>
                L_RL(Œ∏) = -E[(C,œÑ)~D][E[SE~LM_Œ∏(¬∑|C)][r(SE, œÑ, Œ∏)]]
            </div>

            <h2>‚öôÔ∏è Training Approach: ReSTEM</h2>
            <div class="method-box">
                <h3>Why ReSTEM over PPO/GRPO?</h3>
                <ul>
                    <li>PPO and GRPO were unstable in this setting</li>
                    <li>ReSTEM = "Rejection Sampling + SFT"</li>
                    <li>Simpler, more stable approach</li>
                </ul>
                
                <h3>ReSTEM Process:</h3>
                <ol>
                    <li><strong>E-step:</strong> Sample candidate self-edits from current policy</li>
                    <li><strong>M-step:</strong> Reinforce only self-edits with positive reward via SFT</li>
                </ol>
            </div>

            <h2>üèÜ Reward Design</h2>
            <div class="two-column">
                <div>
                    <h3>Binary Reward:</h3>
                    <ul>
                        <li>r = 1 if adaptation improves performance</li>
                        <li>r = 0 otherwise</li>
                    </ul>
                </div>
                <div>
                    <h3>Evaluation Metrics:</h3>
                    <ul>
                        <li><strong>Knowledge:</strong> QA accuracy without passage</li>
                        <li><strong>Few-shot:</strong> Correct task solution</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Slide 7: Experimental Setup -->
        <div class="slide">
            <h1>Experimental Setup</h1>
            
            <h2>üß™ Two Main Evaluations</h2>
            <div class="two-column">
                <div class="challenge-box">
                    <h3>Knowledge Incorporation</h3>
                    <ul>
                        <li><strong>Model:</strong> Qwen2.5-7B</li>
                        <li><strong>Dataset:</strong> SQuAD passages</li>
                        <li><strong>Task:</strong> Answer questions without passage in context</li>
                        <li><strong>Evaluation:</strong> No-context QA accuracy</li>
                        <li><strong>Settings:</strong> Single-passage (n=1) and Continued Pretraining (n=200)</li>
                    </ul>
                </div>
                <div class="challenge-box">
                    <h3>Few-Shot Learning</h3>
                    <ul>
                        <li><strong>Model:</strong> Llama-3.2-1B-Instruct</li>
                        <li><strong>Dataset:</strong> ARC-AGI subset (19 tasks total)</li>
                        <li><strong>Task:</strong> Abstract reasoning from few examples</li>
                        <li><strong>Evaluation:</strong> Success rate on held-out test</li>
                        <li><strong>Tools:</strong> Data augmentations + optimization hyperparameters</li>
                    </ul>
                </div>
            </div>

            <h2>üîß Technical Details</h2>
            <div class="three-column">
                <div>
                    <h3>LoRA Configuration</h3>
                    <ul>
                        <li>Rank: 64-128</li>
                        <li>Alpha: 16-128</li>
                        <li>Efficient updates</li>
                    </ul>
                </div>
                <div>
                    <h3>ReSTEM Training</h3>
                    <ul>
                        <li>2 rounds of RL</li>
                        <li>50 contexts per batch</li>
                        <li>5 self-edits per context</li>
                    </ul>
                </div>
                <div>
                    <h3>Evaluation</h3>
                    <ul>
                        <li>3 random seeds</li>
                        <li>Automated grading</li>
                        <li>Multiple baselines</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Slide 8: Baselines & Tools -->
        <div class="slide">
            <h1>Baselines & Tools</h1>
            
            <h2>üìä Knowledge Incorporation Baselines</h2>
            <div class="method-box">
                <ol>
                    <li><strong>Base Model:</strong> No adaptation or passage access</li>
                    <li><strong>Train on Passage Only:</strong> Direct finetuning on raw passage</li>
                    <li><strong>Train on Passage + Synthetic Data:</strong> Self-generated implications without RL</li>
                    <li><strong>Train on Passage + GPT-4.1 Synthetic Data:</strong> GPT-4.1 generated implications</li>
                    <li><strong>SEAL:</strong> RL-trained self-edit generation</li>
                </ol>
            </div>

            <h2>üéØ Few-Shot Learning Baselines</h2>
            <div class="method-box">
                <ol>
                    <li><strong>ICL (In-Context Learning):</strong> Standard few-shot prompting</li>
                    <li><strong>TTT + Self-Edit (w/o RL):</strong> Test-time training without RL optimization</li>
                    <li><strong>SEAL:</strong> RL-trained tool selection and configuration</li>
                    <li><strong>Oracle TTT:</strong> Optimal human-crafted configuration (upper bound)</li>
                </ol>
            </div>

            <h2>üõ†Ô∏è Available Tools (Few-Shot)</h2>
            <div class="two-column">
                <div>
                    <h3>Data Augmentations:</h3>
                    <ul>
                        <li>Rotations, flips, reflections</li>
                        <li>Transpositions, resizing</li>
                        <li>Chained transformations</li>
                        <li>Repeated transformations</li>
                    </ul>
                </div>
                <div>
                    <h3>Optimization Parameters:</h3>
                    <ul>
                        <li>Learning rate selection</li>
                        <li>Number of training epochs</li>
                        <li>Loss computation strategy</li>
                        <li>Token-level training focus</li>
                    </ul>
                </div>
            </div>
        </div>

        <!-- Slide 9: Detailed Examples from Paper -->
        <div class="slide">
            <h1>Real Examples: Knowledge Incorporation Process</h1>
            
            <h2>üìÑ Input Context (from Figure 5)</h2>
            <div style="background: #f0f8ff; padding: 15px; border-radius: 8px; border: 1px solid #3498db;">
                <strong>Passage:</strong>
                <p style="font-size: 0.9em; line-height: 1.4;">
                "The use of remote sensing for the conservation of the Amazon is also being used by the indigenous tribes of the basin to protect their tribal lands from commercial interests. Using handheld GPS devices and programs like Google Earth, members of the Trio Tribe, who live in the rainforests of southern Suriname, map out their ancestral lands to help strengthen their territorial claims. Currently, most tribes in the Amazon do not have clearly defined boundaries, making it easier for commercial ventures to target their territories."
                </p>
            </div>

            <h2>üîÑ Self-Edit Evolution Across RL Iterations</h2>
            <div class="two-column">
                <div>
                    <h3>Iteration 0 (Before RL):</h3>
                    <div style="background: #fff3cd; padding: 10px; border-radius: 5px; margin: 10px 0; border-left: 4px solid #ffc107;">
                        <p style="font-size: 0.85em; margin: 0;">
                        1. Indigenous tribes can use remote sensing and mapping technology to protect their lands from commercial interests.<br><br>
                        2. Clear boundaries for tribal lands can prevent commercial ventures from targeting and exploiting the resources in their territories.
                        </p>
                    </div>
                </div>
                <div>
                    <h3>Iteration 2 (After RL):</h3>
                    <div style="background: #d4edda; padding: 10px; border-radius: 5px; margin: 10px 0; border-left: 4px solid #27ae60;">
                        <p style="font-size: 0.8em; margin: 0;">
                        1. Remote sensing technology can be used by indigenous tribes to protect their lands.<br>
                        2. The use of GPS devices and programs like Google Earth can help map out ancestral lands.<br>
                        3. Clearing of forests for agricultural land, such as cattle ranches, is a major cause of deforestation in the Amazon.<br>
                        4. Illegal mining for gold and other minerals is a significant threat to the Amazon rainforest.<br>
                        5. The expansion of infrastructure, such as roads and dams, is also a major concern for the conservation of the Amazon.<br>
                        6. The use of remote sensing can help identify areas of the Amazon that are most vulnerable to deforestation and other threats.<br>
                        7. The involvement of indigenous tribes in the protection of the Amazon rainforest is crucial for its conservation.<br>
                        8. The mapping of ancestral lands by indigenous tribes can help strengthen their territorial claims and protect their rights to the land.<br>
                        9. The use of remote sensing can help monitor the Amazon rainforest and track changes over time.<br>
                        10. The protection of the Amazon rainforest is essential for the preservation of biodiversity and the maintenance of ecosystem services.
                        </p>
                    </div>
                </div>
            </div>

            <h2>üìä Evaluation Questions</h2>
            <div style="background: #f8f9fa; padding: 15px; border-radius: 8px; border: 1px solid #6c757d;">
                <p><strong>Q1:</strong> What kind of sending technology is being used to protect tribal lands in the Amazon?</p>
                <p><strong>Q2:</strong> What tribe uses GPS devices to map lands?</p>
                <p><strong>Q3:</strong> Why do some tribes use remote sensing technology?</p>
            </div>

            <div class="highlight">
                <strong>Observation:</strong> RL training leads to more detailed, comprehensive self-edits that result in better downstream performance
            </div>
        </div>

        <!-- Slide 10: Results - Knowledge Incorporation -->
        <div class="slide">
            <h1>Results: Knowledge Incorporation</h1>
            
            <h2>üìà Single-Passage Performance</h2>
            <table class="performance-table">
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Accuracy (%)</th>
                        <th>Improvement</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>Base Model</td>
                        <td>32.7</td>
                        <td>-</td>
                    </tr>
                    <tr>
                        <td>Train on Passage</td>
                        <td>33.5</td>
                        <td>+0.8</td>
                    </tr>
                    <tr>
                        <td>Train on Passage + Synthetic</td>
                        <td>39.7</td>
                        <td>+7.0</td>
                    </tr>
                    <tr>
                        <td>Train on Passage + GPT-4.1 Synthetic</td>
                        <td>46.3</td>
                        <td>+13.6</td>
                    </tr>
                    <tr class="best-result">
                        <td><strong>SEAL</strong></td>
                        <td><strong>47.0</strong></td>
                        <td><strong>+14.3</strong></td>
                    </tr>
                </tbody>
            </table>

            <h2>üéØ Key Findings</h2>
            <div class="result-box">
                <ul>
                    <li><strong>SEAL outperforms GPT-4.1:</strong> 47.0% vs 46.3% despite being much smaller</li>
                    <li><strong>Raw passage training is insufficient:</strong> Only 0.8% improvement</li>
                    <li><strong>Synthetic data is crucial:</strong> Large gains from data augmentation</li>
                    <li><strong>RL provides consistent improvement:</strong> 6-11 percentage points across different prompts</li>
                </ul>
            </div>

            <h2>üìä Continued Pretraining (n=200)</h2>
            <p>Similar ranking maintained: SEAL (43.8%) > GPT-4.1 Synthetic (39.4%) > Base Synthetic (41.0%)</p>
        </div>

        <!-- Slide 11: Results - Few-Shot Learning -->
        <div class="slide">
            <h1>Results: Few-Shot Learning</h1>
            
            <h2>üéØ ARC-AGI Performance</h2>
            <table class="performance-table">
                <thead>
                    <tr>
                        <th>Method</th>
                        <th>Success Rate (%)</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>ICL (In-Context Learning)</td>
                        <td>0</td>
                    </tr>
                    <tr>
                        <td>TTT + Self-Edit (w/o prior RL)</td>
                        <td>20</td>
                    </tr>
                    <tr class="best-result">
                        <td><strong>SEAL</strong></td>
                        <td><strong>72.5</strong></td>
                    </tr>
                    <tr>
                        <td>Oracle TTT (Upper bound)</td>
                        <td>100</td>
                    </tr>
                </tbody>
            </table>

            <h2>üîç Analysis</h2>
            <div class="result-box">
                <h3>Dramatic Improvements:</h3>
                <ul>
                    <li><strong>SEAL vs No RL:</strong> 72.5% vs 20% (+52.5 points)</li>
                    <li><strong>SEAL vs ICL:</strong> 72.5% vs 0% (infinite improvement)</li>
                    <li>Demonstrates automatic tool selection and configuration works</li>
                </ul>
                
                <h3>Room for Improvement:</h3>
                <ul>
                    <li>Still 27.5 points below Oracle TTT</li>
                    <li>Suggests potential for better tool discovery</li>
                    <li>Limited by curated task subset</li>
                </ul>
            </div>
        </div>

        <!-- Slide 12: Limitations & Future Work -->
        <div class="slide">
            <h1>Limitations & Future Directions</h1>
            
            <h2>‚ö†Ô∏è Current Limitations</h2>
            <div class="challenge-box">
                <h3>1. Catastrophic Forgetting</h3>
                <ul>
                    <li>Sequential self-edits lead to performance degradation on earlier tasks</li>
                    <li>No explicit optimization for retention in current setup</li>
                    <li>Performance gradually declines with more updates</li>
                </ul>
            </div>

            <div class="challenge-box">
                <h3>2. Computational Overhead</h3>
                <ul>
                    <li>TTT reward loop is expensive (30-45 seconds per self-edit)</li>
                    <li>Requires finetuning entire model for each reward computation</li>
                    <li>Much more costly than preference-based RL</li>
                </ul>
            </div>

            <div class="challenge-box">
                <h3>3. Context-Dependent Evaluation</h3>
                <ul>
                    <li>Requires explicit downstream tasks for each context</li>
                    <li>Cannot scale to unlabeled corpora easily</li>
                    <li>Limits applicability to general pretraining</li>
                </ul>
            </div>
        </div>

        <!-- Slide 12: Future Vision & Takeaways -->
        <div class="slide">
            <h1>Future Vision & Key Takeaways</h1>
            
            <h2>üöÄ Future Directions</h2>
            <div class="method-box">
                <h3>1. Scaling to Pretraining</h3>
                <ul>
                    <li>Meta-train dedicated SEAL models for synthetic data generation</li>
                    <li>Address the impending "data wall" problem</li>
                    <li>Enable continuous self-improvement without human text</li>
                </ul>
                
                <h3>2. Agentic Systems</h3>
                <ul>
                    <li>Models that adapt dynamically during extended interactions</li>
                    <li>Self-modification based on experience</li>
                    <li>Reduced reliance on repeated supervision</li>
                </ul>
                
                <h3>3. Continual Learning</h3>
                <ul>
                    <li>Reward shaping to penalize forgetting</li>
                    <li>Null-space constrained edits</li>
                    <li>Representational superposition techniques</li>
                </ul>
            </div>

            <h2>üéØ Key Takeaways</h2>
            <div class="highlight">
                <ol>
                    <li><strong>LLMs can learn to self-adapt</strong> by generating their own training data</li>
                    <li><strong>RL-driven self-editing</strong> consistently outperforms static approaches</li>
                    <li><strong>Smaller models can outperform larger ones</strong> with better self-editing</li>
                    <li><strong>Framework is domain-agnostic</strong> and broadly applicable</li>
                    <li><strong>Opens path to truly autonomous learning</strong> systems</li>
                </ol>
            </div>
        </div>

        <div class="navigation">
            <button class="nav-btn" id="prev-btn" onclick="previousSlide()" disabled>Previous</button>
            <button class="nav-btn" id="next-btn" onclick="nextSlide()">Next</button>
        </div>
    </div>

    <script>
        let currentSlide = 0;
        const slides = document.querySelectorAll('.slide');
        const totalSlides = slides.length;
        
        document.getElementById('total-slides').textContent = totalSlides;
        
        function showSlide(n) {
            slides[currentSlide].classList.remove('active');
            currentSlide = n;
            slides[currentSlide].classList.add('active');
            
            document.getElementById('current-slide').textContent = currentSlide + 1;
            
            // Update navigation buttons
            document.getElementById('prev-btn').disabled = currentSlide === 0;
            document.getElementById('next-btn').disabled = currentSlide === totalSlides - 1;
        }
        
        function nextSlide() {
            if (currentSlide < totalSlides - 1) {
                showSlide(currentSlide + 1);
            }
        }
        
        function previousSlide() {
            if (currentSlide > 0) {
                showSlide(currentSlide - 1);
            }
        }
        
        // Keyboard navigation
        document.addEventListener('keydown', function(e) {
            if (e.key === 'ArrowRight' || e.key === ' ') {
                nextSlide();
            } else if (e.key === 'ArrowLeft') {
                previousSlide();
            }
        });
    </script>
</body>
</html>